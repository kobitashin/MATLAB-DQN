%0 -----T 0从最左边出发寻找T，

N_STATES = 6;   % 1维世界的宽度
ACTIONS = {'left','right'};     % 探索者的可用动作
EPSILON = 0.9;   % 贪婪度 greedy
ALPHA = 0.1;     % 学习率
GAMMA = 0.9;    % 奖励递减值
MAX_EPISODES = 23;   % 最大回合数
Q_table=zeros(N_STATES,length(ACTIONS));

for i=1:MAX_EPISODES
    step_counter=0;
    S=1; %回合初始位置
    is_termineated=1;
    update_env(S,i,step_counter,N_STATES);
    while is_termineated
        A=choose_action(S,EPSILON,Q_table,ACTIONS);
        [S_,R]=get_env_feedback(S,A,N_STATES);
        AA=strcmp(A,'right')+1;
        q_predict = Q_table(S,AA);
        if strcmp(S_,'terminal')==0
            q_target=R+GAMMA*max(Q_table(S_,:));
        else
            q_target=R;
            is_termineated=0;
        end
        Q_table(S,AA)=Q_table(S,AA)+ALPHA*(q_target-q_predict);
        S=S_;
        update_env(S,i,step_counter+1,N_STATES);
        step_counter=step_counter+1;
    end
end
disp(Q_table)

function action_name=choose_action(state,EPSILON,Q_table,ACTIONS)
state_action = Q_table(state,:);
if rand()>EPSILON||sum(state_action)==0
    action_name=ACTIONS(randi(2,1));
else
    action_name=ACTIONS(state_action==max(state_action));
end
end

function [S_,R]=get_env_feedback(S,A,N_STATES)
if strcmp(A,'right')
    if S==N_STATES-1
        S_='terminal';
        R=1;
    else
        S_=S+1;
        R=0;
    end
    else
        R=0;
        if S==1
            S_=S;
        else
            S_=S-1;
        end
end
end

function update_env(S,episode,step_counter,N_STATES)
env_list=[repelem({'-'},N_STATES-1) 'T'];
if strcmp(S,'terminal')
    interaction='Episode: %d total_steps = %d \n';
    fprintf(interaction,episode,step_counter);
else
    env_list(S)={'0'};
    disp(cell2mat(env_list));
end
end
